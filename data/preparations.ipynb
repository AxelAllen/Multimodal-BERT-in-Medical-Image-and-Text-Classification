{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arm23_EONUq0"
   },
   "source": [
    "# Data Preparations\n",
    "\n",
    "This notebook goes through the various steps to prepare the data from .csv files to PyTorch Training, Validation, and Test Datasets and Dataloaders.\n",
    "\n",
    "In addition, the ChexNet pre-trained weights that we used had to be processed to be compatible with the current PyTorch architecture. This processing step, which only has to be done once, re-saves the pre-trained weights in a format that can be readily loaded in the current PyTorch version (1.7.0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY0VvSVMOeVN"
   },
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eromxys75SzD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VNKtuKKWcTo"
   },
   "source": [
    "## Checking the Current PyTorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrXWrKz_WOQc",
    "outputId": "e670cf46-df96-4621-b1df-328224bb1732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CbOrFxpOtRU"
   },
   "source": [
    "## Skip Unless running in Google Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVyCYCT46Bet",
    "outputId": "5db5fdf2-915a-467a-c770-68eedd37a281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Vs6paWRDzCAM",
    "outputId": "6e93d1d9-82aa-4fe5-e190-d6da6efe163f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1hEDGBqkUrIWwfe36X07rQ2GZyRNn9uXm/LAP\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/.shortcut-targets-by-id/1hEDGBqkUrIWwfe36X07rQ2GZyRNn9uXm/LAP'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/LAP\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkbw2EGZPMsQ"
   },
   "source": [
    "## Data Sharing\n",
    "\n",
    "We have made the datasets we used in our baseline experiments available via a shared Google Drive as mentioned in the project README.  \n",
    "\n",
    "It is not necessary to re-run the scripts in this notebook to generate the prepared datasets.  \n",
    "\n",
    "However, if you wish, the prepared datasets can be recreated by running the scripts in this notebook.\n",
    "\n",
    "### Data Files Organization\n",
    "\n",
    "* All the original .csv files are in the **'image_labels_csv'** directory. These various csv files are variations of the original dataset in terms of which metadata field(s) we use for the text modality.  \n",
    "\n",
    "* There are 2 metadata fields we consider: \n",
    "    * 'findings'\n",
    "    * 'impression'.  \n",
    "\n",
    "\n",
    "* In Chest X-Ray reporting, in addition to the actual X-ray image, radiologists usually report their 'read' by describing the findings and their determination of the findings. The findings and determination are summarized in the 'findings' and 'impression' fields respectively.  \n",
    "\n",
    "* All the Train, Validation, and Test partitions are saved in their respective directories: **'json'** for the .jsonl files and **'csv'** for the .csv files.\n",
    "\n",
    "* The saved ChexNet pre-trained weights, both the outdated .pth.tar and current .pt versions are in the 'models' directory.\n",
    "\n",
    "* The scripts in the notebook expect that the notebook's **current working directory** is the *same* directory containing this notebook as well as the **'csv', 'json', 'image_labels_csv'**, and **'models'** directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiE3JfgZPtCO"
   },
   "source": [
    "### JSONL Files\n",
    "\n",
    "The MMBT JsonlDataset(Dataset) Class, which prepares the input dataset for generating batches for the model expects the data in a .jsonl file format.  \n",
    "\n",
    "The 'create_jsonl_data' function converts a 'json dict' object to a specified data directory and jsonl filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvf2TvAUR5FK"
   },
   "outputs": [],
   "source": [
    "def create_jsonl_data(data_dir, jsonl_filename, json_dict):\n",
    "    with open(os.path.join(data_dir, jsonl_filename), \"w\") as fw:\n",
    "        data_obj = {}\n",
    "        for idx, data_dict in json_dict.items():\n",
    "            data_obj['id'] = idx\n",
    "            for key, value in data_dict.items():\n",
    "                data_obj[key] = value\n",
    "            fw.write(\"%s\\n\" % json.dumps(data_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ORpsXMPQTu_"
   },
   "source": [
    "### .CSV Files\n",
    "\n",
    "The text and image only models do not need the data to be in the .jsonl file format.  \n",
    "\n",
    "The 'creat_csv_data' simply saves a Pandas dataframe to a specified data directory and csv filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4eY4OKSviiu"
   },
   "outputs": [],
   "source": [
    "def create_csv_data(data_dir, csv_filename, df):\n",
    "    df.to_csv(path_or_buf=os.path.join(data_dir, csv_filename))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swzBY2rMQ5Bo"
   },
   "source": [
    "### Train, Validation, and Test Set Partitioning\n",
    "\n",
    "We partition all the original csv files into a 60/20/20 Train/Validation/Test partitions.  \n",
    "\n",
    "We use the standard *sklearn train_test_split* function with the specified *random_state = 1* to create the partitions with reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cumVQblFZ-uZ"
   },
   "source": [
    "#### Specifying Directory Names\n",
    "\n",
    "These are where the current original csv and prepared jsonl and csv datasets are stored in the shared drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__file__ does not exist for notebook, use current directory instead\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    FILE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    print('__file__ does not exist for notebook, use current directory instead')\n",
    "    FILE_DIR = Path().resolve()\n",
    "    \n",
    "print(f'current directory is: {FILE_DIR}')    \n",
    "ORIG_DATA_DIR = 'image_labels_csv'\n",
    "JSON_DIR = 'json'\n",
    "CSV_DIR = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iO1eqLuxAvbD",
    "outputId": "38487ce1-57d7-49b8-d8f9-75253547a072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences in image_labels_findings: 6,461\n",
      "\n",
      "                        img  ...                                               text\n",
      "0   CXR1_1_IM-0001-3001.png  ...   The cardiac silhouette and mediastinum size a...\n",
      "1   CXR1_1_IM-0001-4001.png  ...   The cardiac silhouette and mediastinum size a...\n",
      "2     CXR2_IM-0652-1001.png  ...   Borderline cardiomegaly. Midline sternotomy X...\n",
      "3     CXR2_IM-0652-2001.png  ...   Borderline cardiomegaly. Midline sternotomy X...\n",
      "4  CXR5_IM-2117-1003002.png  ...   The cardiomediastinal silhouette and pulmonar...\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "\n",
      "Number of training sentences in image_labels_findings_frontal: 2,847\n",
      "\n",
      "                      img  ...                                               text\n",
      "0   CXR2_IM-0652-1001.png  ...   Borderline cardiomegaly. Midline sternotomy X...\n",
      "1   CXR6_IM-2192-1001.png  ...   Heart size and mediastinal contour are within...\n",
      "2   CXR8_IM-2333-1001.png  ...   The heart, pulmonary XXXX and mediastinum are...\n",
      "3  CXR10_IM-0002-1001.png  ...   The cardiomediastinal silhouette is within no...\n",
      "4  CXR11_IM-0067-1001.png  ...   Cardiomediastinal silhouette and pulmonary va...\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "\n",
      "Number of training sentences in image_labels_both_frontal: 3,246\n",
      "\n",
      "                      img  ...                                               text\n",
      "0   CXR2_IM-0652-1001.png  ...   Borderline cardiomegaly. Midline sternotomy X...\n",
      "1   CXR6_IM-2192-1001.png  ...   Heart size and mediastinal contour are within...\n",
      "2   CXR8_IM-2333-1001.png  ...   The heart, pulmonary XXXX and mediastinum are...\n",
      "3  CXR10_IM-0002-1001.png  ...   The cardiomediastinal silhouette is within no...\n",
      "4  CXR11_IM-0067-1001.png  ...   Cardiomediastinal silhouette and pulmonary va...\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "\n",
      "Number of training sentences in image_labels_both: 7,418\n",
      "\n",
      "                        img  ...                                               text\n",
      "0   CXR1_1_IM-0001-3001.png  ...   The cardiac silhouette and mediastinum size a...\n",
      "1   CXR1_1_IM-0001-4001.png  ...   The cardiac silhouette and mediastinum size a...\n",
      "2     CXR2_IM-0652-1001.png  ...   Borderline cardiomegaly. Midline sternotomy X...\n",
      "3     CXR2_IM-0652-2001.png  ...   Borderline cardiomegaly. Midline sternotomy X...\n",
      "4  CXR5_IM-2117-1003002.png  ...   The cardiomediastinal silhouette and pulmonar...\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "\n",
      "Number of training sentences in image_labels_impression: 7,418\n",
      "\n",
      "                        img  label                                   text\n",
      "0   CXR1_1_IM-0001-3001.png      0                   normal chest x-xxxx.\n",
      "1   CXR1_1_IM-0001-4001.png      0                   normal chest x-xxxx.\n",
      "2     CXR2_IM-0652-1001.png      0           no acute pulmonary findings.\n",
      "3     CXR2_IM-0652-2001.png      0           no acute pulmonary findings.\n",
      "4  CXR5_IM-2117-1003002.png      0  no acute cardiopulmonary abnormality.\n",
      "\n",
      "Number of training sentences in image_labels_impression_frontal: 3,246\n",
      "\n",
      "                      img  label                                text\n",
      "0   CXR2_IM-0652-1001.png      0        no acute pulmonary findings.\n",
      "1   CXR6_IM-2192-1001.png      0  no acute cardiopulmonary findings.\n",
      "2   CXR8_IM-2333-1001.png      0   no acute cardiopulmonary disease.\n",
      "3  CXR10_IM-0002-1001.png      0   no acute cardiopulmonary process.\n",
      "4  CXR11_IM-0067-1001.png      0  no acute cardiopulmonary findings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets into a pandas dataframe.\n",
    "csv_files = []\n",
    "path = os.path.join(FILE_DIR, ORIG_DATA_DIR)\n",
    "for filename in os.listdir(path):\n",
    "    name = filename.partition('.')[0]\n",
    "    df = pd.read_csv(os.path.join(path, filename))\n",
    "    df = df.rename(columns={\"Filename\": \"img\", \"Label\": \"label\", \"LabelText\": \"text\"})\n",
    "    csv_files.append((name, df))\n",
    "\n",
    "# Report the number of sentences and print out the first few examples.\n",
    "for df in csv_files:\n",
    "    print(f'Number of training sentences in {df[0]}: {df[1].shape[0]:,}\\n')\n",
    "    print(f'{df[1].head()}\\n')\n",
    "\n",
    "# Split dataframes into train/val/test and create jsonl/csv files from splits.\n",
    "for df in csv_files:\n",
    "    train, test = train_test_split(df[1], test_size=0.2, random_state=1)\n",
    "    \n",
    "    # 0.25 * 0.8 = 0.2\n",
    "    # end result: 60/20/20 train/val/test splits\n",
    "    train, val = train_test_split(train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    # csv files\n",
    "    create_csv_data(CSV_DIR, df[0]+'_train.csv', train)\n",
    "    create_csv_data(CSV_DIR, df[0]+'_val.csv', val)\n",
    "    create_csv_data(CSV_DIR, df[0]+'_test.csv', test)\n",
    "    \n",
    "    # jsonl files\n",
    "    train_json_str = train.to_json(orient=\"index\")\n",
    "    train_df_json = json.loads(train_json_str)\n",
    "    \n",
    "    val_json_str = val.to_json(orient='index')\n",
    "    val_df_json = json.loads(val_json_str)\n",
    "    \n",
    "    test_json_str = test.to_json(orient='index')\n",
    "    test_df_json = json.loads(test_json_str)\n",
    "    \n",
    "    create_jsonl_data(JSON_DIR, df[0]+'_train.jsonl', train_df_json)\n",
    "    create_jsonl_data(JSON_DIR, df[0]+'_val.jsonl', val_df_json)\n",
    "    create_jsonl_data(JSON_DIR, df[0]+'_test.jsonl', test_df_json)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqEnEoxTVObP"
   },
   "source": [
    "## ChexNet Saved Pre-Trained Weight\n",
    "\n",
    "For convenience, we did not re-implement nor re-trained the ChexNet experiment to obtain the weight, but we used the\n",
    "weights from the following PyTorch implementation: https://github.com/arnoweng/CheXNet/\n",
    "\n",
    "### Specifying Directories and ChexNet parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JCg86VxHcCcI"
   },
   "outputs": [],
   "source": [
    "SAVED_MODELS_DIR = 'models'\n",
    "CKPT_PATH = os.path.join(SAVED_MODELS_DIR, 'model.pth.tar')\n",
    "N_CLASSES = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ax_w-mS6CJe"
   },
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYZiD_Ml6NGn",
    "outputId": "7b9e7929-0f6a-4216-d97f-213cff98b3c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "=> loaded checkpoint\n"
     ]
    }
   ],
   "source": [
    "# initialize and load the model\n",
    "# comment out .cuda for when using the weights as part of MMBT\n",
    "model_ft = DenseNet121(N_CLASSES)#.cuda()\n",
    "\n",
    "# since the pickled pretrained model was created using a much older version of\n",
    "# pytorch we need to reformat the file a bit to be able to open it with current\n",
    "# pytorch version.\n",
    "if os.path.isfile(CKPT_PATH):\n",
    "    print(\"=> loading checkpoint\")\n",
    "    checkpoint = torch.load(CKPT_PATH, map_location=torch.device('cpu'))\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    remove_data_parallel = True\n",
    "    pattern = re.compile(\n",
    "              r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "\n",
    "    for key in list(state_dict.keys()):\n",
    "        match = pattern.match(key)\n",
    "        new_key = match.group(1) + match.group(2) if match else key\n",
    "        new_key = new_key[7:] if remove_data_parallel else new_key\n",
    "        state_dict[new_key] = state_dict[key]\n",
    "        # Delete old key only if modified.\n",
    "        if match or remove_data_parallel: \n",
    "            del state_dict[key]\n",
    "\n",
    "    model_ft.load_state_dict(state_dict)\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    print(\"=> no checkpoint found\")\n",
    "\n",
    "\n",
    "torch.save(model_ft.densenet121, os.path.join(SAVED_MODELS_DIR,'saved_chexnet.pt'))\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE75XshWdgs3"
   },
   "source": [
    "# Skip unless running in Google Colab and Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxAkJGOzMKHG",
    "outputId": "36126b3a-5a74-464c-b68f-ae0f6ac80fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All changes made in this colab session should now be visible in Drive.\n"
     ]
    }
   ],
   "source": [
    "drive.flush_and_unmount()\n",
    "print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preparations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}